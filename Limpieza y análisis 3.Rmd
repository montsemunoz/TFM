---
title: "Limpiez y análisis exploratorio"
author: "Montse Muñoz Aragón"
date: "1/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r include=FALSE, warning=FALSE}
# install.packages("stringr")
# install.packages("xlsx")
# install.packages("devtools")
# install.packages("stopwords")
# install.packages("tm")
# install.packages("quanteda")
# install.packages("e1071")
#install.packages("wordcloud")
#install.packages("RColorBrewer")
# install.packages ("ggraph")
# install.packages("topicmodels")
library(stringr)
library(devtools)
library(dplyr)
library(purrr)
library(tidyr)
library(stopwords)
library(tm)
library(quanteda)
library(e1071)
library(readxl)
library(lubridate)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(gridExtra)
library(scales)
library(tidytext)
library(igraph)
library(ggraph)
library(forcats)
library(topicmodels)
library(textmineR)
library(BullsEyeR)
```

## Lectura de datos

```{r warning=FALSE}
datos <- read.csv2("Cuentas Oficiales y Personales.csv")
head(datos)
nrow(datos)
```

```{r}
# Se renombran las variables con partidos más prácticos
datos <- datos %>% rename(user_id = user_id, status_id=status_id, fecha = created_at,
                            partido = screen_name, tweet = text, retweets=retweet_count)
head(datos)
```

## Juntar partidos y presidentes

```{r}
datos <- datos %>%
  mutate(partido = fct_recode(partido,
    "vox_es" = "Santi_ABASCAL",
    "populares" = "pablocasado_",
    "CiudadanosCs" = "InesArrimadas",
    "PSOE" = "sanchezcastejon",
    "PODEMOS" = "PabloIglesias",
    "Esquerra_ERC" = "perearagones",
    "JuntsXCat" = "LauraBorras",
    "eajpnv" = "AITOR_ESTEBAN",
    "ehbildu" = "ArnaldoOtegi"))
datos %>% count(partido)
```

```{r}
datosvox <- datos %>% filter(datos$partido=="vox_es")
datospp <- datos %>% filter(datos$partido=="populares")
datoscs <- datos %>% filter(datos$partido=="CiudadanosCs")
datospsoe <- datos %>% filter(datos$partido=="PSOE")
datospodemos <- datos %>% filter(datos$partido=="PODEMOS")
datoserc <- datos %>% filter(datos$partido=="Esquerra_ERC")
datosjpc <- datos %>% filter(datos$partido=="JuntsXCat")
datospnv <- datos %>% filter(datos$partido=="eajpnv")
datosbildu <- datos %>% filter(datos$partido=="ehbildu")

vector <- as.factor(c("datosvox","datospp","datoscs","datospsoe","datospodemos","datoserc","datosjpc","datospnv","datosbildu"))
```

## Limpieza

```{r}
limpiar_tokenizar <- function(texto){
  # El orden de la limpieza no es arbitrario
    # Se convierte todo el texto a minúsculas
    nuevo_texto <- tolower(texto)
    # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
    # de cualquier cosa que no sea un espacio)
    nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
    # Eliminación de signos de puntuación
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
    # Eliminación de números
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
    # Eliminación de espacios en blanco múltiples
    nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
    # Eliminación de menciones
    nuevo_texto <- str_replace_all(nuevo_texto,"@\\w+", " ")
    # Tokenización por palabras individuales
    nuevo_texto <- str_split(nuevo_texto, " ")[[1]]
    # Eliminación de tokens con una longitud < 2
    nuevo_texto <- keep(nuevo_texto, .p = function(x){str_length(x) > 2})
    return(nuevo_texto)
    # No repetir tweets
    nuevo_texto <- unique(nuevo_texto)
}

tweets <- datos %>% mutate(texto_tokenizado = map(.x = datos$tweet, .f = limpiar_tokenizar))
tweets %>% select(texto_tokenizado) %>% head()
tweets %>% slice(1) %>% select(texto_tokenizado) %>% pull()
```

```{r}
head(tweets)
tweets_tidy <- tweets %>% select(-tweet)%>%unnest(cols=c(texto_tokenizado))
tweets_tidy <- tweets_tidy %>% rename(token = texto_tokenizado)
head(tweets_tidy) 
```

## Extracción de características

```{r}
lista_stopwords<-c('a','abans','abintestat','ací','adesiara','adés','adéu','adàgio','ah','ahir','ai','aitambé','aitampoc','aitan','aitant',
                   'aitantost','aixà','això','així','aleshores','algun','alguna','algunes','alguns','algú','alhora','allà','allèn','allò',
                   'allí','almenys','alto','altra','altre','altres','altresí','altri','alça','al·legro','amargament','amb','ambdues','ambdós',
                   'amunt','amén','anc','andante','andantino','anit','ans','antany','apa','aprés','aqueix','aqueixa','aqueixes','aqueixos',
                   'aqueixs','aquell','aquella','aquelles','aquells','aquest','aquesta','aquestes','aquests','aquèn','aquí','ara','arran',
                   'arrera','arrere','arreu','arri','arruix','atxim','au','avall','avant','aviat','avui','açò','bah','baix','baldament',
                   'ballmanetes','banzim-banzam','bastant','bastants','ben','bis','bitllo-bitllo','bo','bé','ca','cada','cal','cap','car',
                   'caram','catorze','cent','centes','cents','cerca','cert','certa','certes','certs','cinc','cinquanta','cinquena','cinquenes',
                   'cinquens','cinquè','com','comsevulla','contra','cordons','corrents','cric-crac','d','daixonses','daixò','dallonses','dallò',
                   'dalt','daltabaix','damunt','darrera','darrere','davall','davant','de','debades','dedins','defora','dejorn','dejús','dellà',
                   'dementre','dempeus','demés','demà','des','desena','desenes','desens','després','dessobre','dessota','dessús','desè','deu',
                   'devers','devora','deçà','diferents','dinou','dins','dintre','disset','divers','diversa','diverses','diversos','divuit','doncs',
                   'dos','dotze','dues','durant','ecs','eh','el','ela','elis','ell','ella','elles','ells','els','em','emperò','en','enans','enant',
                   'encara','encontinent','endalt','endarrera','endarrere','endavant','endebades','endemig','endemés','endemà','endins','endintre',
                   'enfora','engir','enguany','enguanyasses','enjús','enlaire','enlloc','enllà','enrera','enrere','ens','ensems','ensota','ensús',
                   'entorn','entre','entremig','entretant','entrò','envers','envides','environs','enviró','ençà','ep','ep','era','eren','eres',
                   'ergo','es','escar','essent','esser','est','esta','estada','estades','estan','estant','estar','estaran','estarem','estareu',
                   'estaria','estarien','estaries','estaré','estarà','estaràs','estaríem','estaríeu','estat','estats','estava','estaven','estaves',
                   'estem','estes','esteu','estic','estiguem','estigueren','estigueres','estigues','estiguessis','estigueu','estigui','estiguin',
                   'estiguis','estigué','estiguérem','estiguéreu','estigués','estiguí','estos','està','estàs','estàvem','estàveu','et','etc','etcètera',
                   'ets','excepte','fins','fora','foren','fores','força','fos','fossin','fossis','fou','fra','fui','fóra','fórem','fóreu','fóreu',
                   'fóssim','fóssiu','gaire','gairebé','gaires','gens','girientorn','gratis','ha','hagi','hagin','hagis','haguda','hagudes','hagueren',
                   'hagueres','haguessin','haguessis','hagut','haguts','hagué','haguérem','haguéreu','hagués','haguéssim','haguéssiu','haguí','hala',
                   'han','has','hauran','haurem','haureu','hauria','haurien','hauries','hauré','haurà','hauràs','hauríem','hauríeu','havem','havent',
                   'haver','haveu','havia','havien','havies','havíem','havíeu','he','hem','heu','hi','ho','hom','hui','hàgim','hàgiu','i','igual','iguals',
                   'inclusive','ja','jamai','jo','l','la','leri-leri','les','li','lla','llavors','llevat','lluny','llur','llurs','lo','los','ls','m','ma',
                   'mai','mal','malament','malgrat','manco','mant','manta','mantes','mantinent','mants','massa','mateix','mateixa','mateixes','mateixos',
                   'me','mentre','mentrestant','menys','mes','meu','meua','meues','meus','meva','meves','mi','mig','mil','mitges','mitja','mitjançant',
                   'mitjos','moixoni','molt','molta','moltes','molts','mon','mos','més','n','na','ne','ni','ningú','no','nogensmenys','només','noranta',
                   'nos','nosaltres','nostra','nostre','nostres','nou','novena','novenes','novens','novè','ns','nòs','nós','o','oh','oi','oidà','on','onsevulga',
                   'onsevulla','onze','pas','pengim-penjam','per','perquè','pertot','però','piano','pla','poc','poca','pocs','poques','potser','prest','primer',
                   'primera','primeres','primers','pro','prompte','prop','prou','puix','pus','pàssim','qual','quals','qualsevol','qualsevulla','qualssevol',
                   'qualssevulla','quan','quant','quanta','quantes','quants','quaranta','quart','quarta','quartes','quarts','quasi','quatre','que','quelcom',
                   'qui','quin','quina','quines','quins','quinze','quisvulla','què','ran','re','rebé','renoi','rera','rere','res','retruc','s','sa','salvament',
                   'salvant','salvat','se','segon','segona','segones','segons','seguida','seixanta','sempre','sengles','sens','sense','ser','seran','serem',
                   'sereu','seria','serien','series','seré','serà','seràs','seríem','seríeu','ses','set','setanta','setena','setenes','setens','setze','setè',
                   'seu','seua','seues','seus','seva','seves','si','sia','siau','sic','siguem','sigues','sigueu','sigui','siguin','siguis','sinó','sis','sisena',
                   'sisenes','sisens','sisè','sobre','sobretot','sol','sola','solament','soles','sols','som','son','sos','sota','sots','sou','sovint','suara',
                   'sí','sóc','són','t','ta','tal','tals','també','tampoc','tan','tanmateix','tant','tanta','tantes','tantost','tants','te','tercer','tercera',
                   'terceres','tercers','tes','teu','teua','teues','teus','teva','teves','ton','tos','tost','tostemps','tot','tota','total','totes','tothom',
                   'tothora','tots','trenta','tres','tret','tretze','tu','tururut','u','uf','ui','uix','ultra','un','una','unes','uns','up','upa','us','va',
                   'vagi','vagin','vagis','vaig','vair','vam','van','vares','vas','vau','vem','verbigràcia','vers','vet','veu','vint','vora','vos','vosaltres',
                   'vostra','vostre','vostres','vostè','vostès','vuit','vuitanta','vuitena','vuitenes','vuitens','vuitè','vés','vàreig','vàrem','vàreu','vós',
                   'xano-xano','xau-xau','xec','érem','éreu','és','ésser','àdhuc','àlies','ça','ço','òlim','ídem','últim','última','últimes','últims','únic','única','únics','úniques','dels','fer')
```

```{r}
# Catalán
stopwords_cat <- read.table("https://raw.githubusercontent.com/stopwords-iso/stopwords-ca/master/stopwords-ca.txt", encoding = "UTF-8", stringsAsFactors = F)
stopwords_cat <- stopwords_cat[,1]
stopwords_cat <- c(stopwords_cat, lista_stopwords)

#Español
stopwords_es <- stopwords("spanish")
stopwords <- c(stopwords_cat, stopwords_es,"lauraborras","perearagones","inesarrimadas", "sanchezcastejon","santiabascal","abascal","santi", "pablocasado","arnaldootegui","aitoresteban","esteban","pabloiglesias", "hoy", "eajpnv","ehbildu","juntsxcat","esquerra","erc","podemos","populares","vox_es","vox","ciudadanoscs","psoe","sanchez","sánchez", "psc")
```

```{r}
tweets_tidy <- tweets_tidy %>% filter(!(token %in% stopwords))
```

## Análisis exploratorio 

```{r}
ggplot(tweets, aes(x = as.Date(fecha), fill = partido)) +
  geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +
  scale_x_date(date_labels = "%d-%m") +
  labs(x = "Fecha de publicación", y = "Número de tweets") +
  facet_wrap(~ partido, ncol = 4) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90))
```

```{r}
tweets$fecha <- as.Date(tweets$fecha)
tweets_fecha <- tweets %>% mutate(fecha = format(fecha, "%Y-%d-%m"))
tweets_fecha %>% group_by(partido, fecha) %>% summarise(n = n()) %>%
  ggplot(aes(x = fecha, y = n, color = partido)) +
  geom_line(aes(group = partido)) +
  labs(title = "Número de tweets publicados", x = "Fecha de publicación",
       y = "Número de tweets") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, size = 6),
        legend.position = "bottom")
```

## Total de palabras utilizadas por cada usuario 

```{r}
tweets_tidy %>% group_by(partido) %>% summarise(n = n()) 
```

```{r}
tweets_tidy %>%  ggplot(aes(x = partido)) + geom_bar() + coord_flip() + theme_bw() 
```

## Palabras distintas utilizadas por cada usuario

```{r}
tweets_tidy %>% select(partido, token) %>% distinct() %>%  group_by(partido) %>% summarise(palabras_distintas = n())
```

```{r}
tweets_tidy %>% select(partido, token) %>% distinct() %>%ggplot(aes(x = partido)) + geom_bar() + coord_flip() + theme_bw()
```

## Longitud media de los tweets por usuario

```{r}
tweets_tidy %>% group_by(partido, status_id) %>% summarise(longitud = n()) %>% group_by(partido) %>% summarise(media_longitud = mean(longitud), sd_longitud = sd(longitud))
```

```{r}
tweets_tidy %>% group_by(partido, status_id) %>% summarise(longitud = n()) %>% group_by(partido) %>%
                summarise(media_longitud = mean(longitud),
                          sd_longitud = sd(longitud)) %>%
                ggplot(aes(x = partido, y = media_longitud)) +
                geom_col() +
                geom_errorbar(aes(ymin = media_longitud - sd_longitud,
                                  ymax = media_longitud + sd_longitud)) +
                coord_flip() + theme_bw()
```

## Palabras más utilizadas por usuario

```{r}
tweets_tidy %>% group_by(partido, token) %>% count(token) %>% group_by(partido) %>% top_n(10, n) %>% arrange(partido, desc(n)) %>% print(n=30)
```

## Representación gráfica de las frecuencias

```{r}
tweets_tidy %>% group_by(partido, token) %>% count(token) %>% group_by(partido) %>%
                top_n(10, n) %>% arrange(partido, desc(n)) %>%
                ggplot(aes(x = reorder(token,n), y = n, fill = partido)) +
                geom_col() +
                theme_bw() +
                labs(y = "", x = "") +
                theme(legend.position = "none") +
                coord_flip() +
                facet_wrap(~partido,scales = "free", ncol = 4, drop = TRUE)
```

## Wordcloud

```{r warning=F}
wordcloud_custom <- function(grupo, df){
  print(grupo)
  wordcloud(words = df$token, freq = df$frecuencia,
            max.words = 300, random.order = FALSE, rot.per = 0.35,
            colors = brewer.pal(7, "Dark2"))
}

df_grouped <- tweets_tidy %>% group_by(partido, token) %>% count(token) %>%
              group_by(partido) %>% mutate(frecuencia = n / n()) %>%
              arrange(partido, desc(frecuencia)) %>% nest() 

walk2(.x = df_grouped$partido, .y = df_grouped$data, .f = wordcloud_custom)
```

## Correlación entre usuarios por palabras utilizadas

```{r}
tweets_spread <- tweets_tidy %>% group_by(partido, token) %>% count(token) %>% spread(key = partido, value = n, fill = NA, drop = TRUE)

cor.test(~ vox_es + populares, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ vox_es + PODEMOS, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ vox_es + ehbildu, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ PSOE + populares, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ vox_es + JuntsXCat, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ Esquerra_ERC + JuntsXCat, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ PSOE + Esquerra_ERC, method = "pearson", data = tweets_spread)
```

```{r}
cor.test(~ vox_es + Esquerra_ERC, method = "pearson", data = tweets_spread)
```

```{r}
p1 <- ggplot(tweets_spread, aes(vox_es, JuntsXCat)) +
      geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
      geom_text(aes(label = token), check_overlap = TRUE, vjust = 1.5) +
      scale_x_log10(labels = percent_format()) +
      scale_y_log10(labels = percent_format()) +
      geom_abline(color = "red") +
      theme_bw() +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank())

p2 <- ggplot(tweets_spread, aes(vox_es, populares)) +
      geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +
      geom_text(aes(label = token), check_overlap = TRUE, vjust = 1.5) +
      scale_x_log10(labels = percent_format()) +
      scale_y_log10(labels = percent_format()) +
      geom_abline(color = "red") +
      theme_bw() +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank())

grid.arrange(p1, p2, nrow = 1)
```


```{r}
palabras_comunes <- dplyr::intersect(tweets_tidy %>% filter(partido=="vox_es") %>%
                    select(token), tweets_tidy %>% filter(partido=="JuntsXCat") %>%
                    select(token)) %>% nrow()
paste("Número de palabras comunes entre Vox y Junts per Catalunya", palabras_comunes)
```

```{r}
palabras_comunes <- dplyr::intersect(tweets_tidy %>% filter(partido=="vox_es") %>%
                    select(token), tweets_tidy %>% filter(partido=="populares") %>%
                    select(token)) %>% nrow()
paste("Número de palabras comunes entre Vox y PP", palabras_comunes)
```

### Palabras se utilizan de forma más diferenciada por cada usuario

```{r}
# Pivotaje y despivotaje
tweets_spread <- tweets_tidy %>% group_by(partido, token) %>% count(token) %>%
                 spread(key = partido, value = n, fill = 0, drop = TRUE)
tweets_unpivot <- tweets_spread %>% gather(key = "partido", value = "n", -token)

# Selección de los autores elonmusk y mayoredlee
tweets_unpivot <- tweets_unpivot %>% filter(partido %in% c("vox_es",
                                                         "JuntsXCat"))
# Se añade el total de palabras de cada autor
tweets_unpivot <- tweets_unpivot %>% left_join(tweets_tidy %>%
                                               group_by(partido) %>%
                                               summarise(N = n()),
                                               by = "partido")
# Cálculo de odds y log of odds de cada palabra
tweets_logOdds <- tweets_unpivot %>%  mutate(odds = (n + 1) / (N + 1))
tweets_logOdds <- tweets_logOdds %>% select(partido, token, odds) %>% 
                  spread(key = partido, value = odds)
tweets_logOdds <- tweets_logOdds %>%  mutate(log_odds = log(vox_es/JuntsXCat),
                                             abs_log_odds = abs(log_odds))
# Si el logaritmo de odds es mayor que cero, significa que es una palabra con
# mayor probabilidad de ser de Elon Musk. Esto es así porque el ratio sea ha
# calculado como elonmusk/mayoredlee.
tweets_logOdds <- tweets_logOdds %>%
                  mutate(autor_frecuente = if_else(log_odds > 0,
                                                   "@vox_es",
                                                   "@JuntsXCat"))
tweets_logOdds %>% arrange(desc(abs_log_odds)) %>% head() 
```

```{r}
tweets_logOdds %>% group_by(autor_frecuente) %>% top_n(15, abs_log_odds) %>% ggplot(aes(x = reorder(token, log_odds), y = log_odds, fill = autor_frecuente)) + geom_col() + labs(x = "palabra", y = "log odds ratio (@vox_es / JuntsXCat)") + coord_flip() +theme_bw()
```

## Relación entre palabras

```{r}
limpiar<- function(texto){
  # El orden de la limpieza no es arbitrario
    # Se convierte todo el texto a minúsculas
    nuevo_texto <- tolower(texto)
    # Eliminación de páginas web (palabras que empiezan por "http." seguidas 
    # de cualquier cosa que no sea un espacio)
    nuevo_texto <- str_replace_all(nuevo_texto,"http\\S*", "")
    # Eliminación de signos de puntuación
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:punct:]]", " ")
    # Eliminación de números
    nuevo_texto <- str_replace_all(nuevo_texto,"[[:digit:]]", " ")
    # Eliminación de espacios en blanco múltiples
    nuevo_texto <- str_replace_all(nuevo_texto,"[\\s]+", " ")
    # Eliminación de menciones
    nuevo_texto <- str_replace_all(nuevo_texto,"@\\w+", " ")
}

bigramas <- datos %>% mutate(texto = limpiar(datos$tweet)) %>% select(texto) %>% unnest_tokens(input = texto, output = "bigrama", token = "ngrams", n = 2, drop = TRUE)

# Contaje de ocurrencias de cada bigrama
bigramas  %>% count(bigrama, sort = TRUE)
```

```{r}
# Separación de los bigramas 
bigrams_separados <- bigramas %>% separate(bigrama, c("palabra1", "palabra2"), sep = " ")
head(bigrams_separados)
```

```{r}
# Filtrado de los bigramas que contienen alguna stopword
bigrams_separados <- bigrams_separados  %>% filter(!palabra1 %in% stopwords) %>% filter(!palabra2 %in% stopwords)

# Unión de las palabras para formar de nuevo los bigramas
bigramas <- bigrams_separados %>% unite(bigrama, palabra1, palabra2, sep = " ")

# Nuevo contaje para identificar los bigramas más frecuentes
bigramas  %>% count(bigrama, sort = TRUE) %>% print(n = 20)
```

```{r}
graph <- bigramas %>% separate(bigrama, c("palabra1", "palabra2"), sep = " ") %>% count(palabra1, palabra2, sort = TRUE) %>%
filter(n > 50) %>% graph_from_data_frame(directed = FALSE)

set.seed(123)

plot(graph, vertex.label.font = 2, vertex.label.color = "black", vertex.label.cex = 0.7, edge.color = "gray85")
```

```{r}
ggraph(graph = graph) + geom_edge_link(colour = "gray70") + geom_node_text(aes(label = name), size = 4) + theme_bw()
```

```{r}
corpus <- Corpus(VectorSource(tweets_tidy$token))

#doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpus)))
dtm <- DocumentTermMatrix(corpus)
```

```{r}
#create DTM
dtm <- createDTM(ftweets)

#explore the basic frequency
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq,doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Eliminate words appearing less than 2 times or in more than half of the
# documents
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm = dtm
```

## MODELO LDA

```{r}
#LDA model with 5 topics selected
lda_5 = LDA(dtm, k = 5, method = 'Gibbs', control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
lda_2 = LDA(dtm, k = 2, method = 'Gibbs', control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
lda_10 = LDA(dtm, k = 10, method = 'Gibbs', control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, thin = 500, burnin = 4000, iter = 2000))
```

## Probabilidad de que una palabra pertenezca a un tópico

```{r}
lda_td1 <- tidy(lda_2, matrix = "beta")
lda_td2 <- tidy(lda_5, matrix = "beta")
lda_td3 <- tidy(lda_10, matrix = "beta")
```

```{r}
terminos_frecuentes1 <- lda_td1 %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes2 <- lda_td2 %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes3 <- lda_td3 %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
```

## Probabilidad de que un tweet pertenzeca a un tópico

```{r}
lda_gamma1 <- tidy(lda_2, matrix = "gamma")
lda_gamma2 <- tidy(lda_5, matrix = "gamma")
lda_gamma3 <- tidy(lda_10, matrix = "gamma")
```

## POR PARTIDO

```{r}
datosvox <- tweets_tidy %>% filter(tweets_tidy$partido=="vox_es")
datospp <- tweets_tidy %>% filter(tweets_tidy$partido=="populares")
datoscs <- tweets_tidy %>% filter(tweets_tidy$partido=="CiudadanosCs")
datospsoe <- tweets_tidy %>% filter(tweets_tidy$partido=="PSOE")
datospodemos <- tweets_tidy %>% filter(tweets_tidy$partido=="PODEMOS")
datoserc <- tweets_tidy %>% filter(tweets_tidy$partido=="Esquerra_ERC")
datosjpc <- tweets_tidy %>% filter(tweets_tidy$partido=="JuntsXCat")
datospnv <- tweets_tidy %>% filter(tweets_tidy$partido=="eajpnv")
datosbildu <- tweets_tidy %>% filter(tweets_tidy$partido=="ehbildu")
```

#### VOX

```{r}
corpusv <- Corpus(VectorSource(datosvox$token))

doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpusv)))
dtmv <- DocumentTermMatrix(corpusv[doc.lengths > 0]) #documents== Tweets, terms== palabras
```

```{r}
#LDA model with 5 topics selected
lda_5v = LDA(dtmv, k = 5, method = 'Gibbs', control = list(seed = list(1505)))

#LDA model with 2 topics selected
lda_2v = LDA(dtmv, k = 2, method = 'Gibbs', control = list(seed = list(1505)))

#LDA model with 10 topics selected
lda_10v = LDA(dtmv, k = 10, method = 'Gibbs', control = list(seed = list(1505)))
```

## Probabilidad de que una palabra pertenezca a un tópico

```{r}
lda_td1v <- tidy(lda_2v, matrix = "beta")
lda_td2v <- tidy(lda_5v, matrix = "beta")
lda_td3v <- tidy(lda_10v, matrix = "beta")
```

```{r}
terminos_frecuentes1v <- lda_td1v %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes2v <- lda_td2v %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes3v <- lda_td3v %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
```

## Probabilidad de que un tweet pertenzeca a un tópico

```{r}
lda_gamma1v <- tidy(lda_2v, matrix = "gamma")
lda_gamma2v <- tidy(lda_5v, matrix = "gamma")
lda_gamma3v <- tidy(lda_10v, matrix = "gamma")
```

#### PP

```{r}
corpuspp <- Corpus(VectorSource(datospp$token))

doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpuspp)))
dtmpp <- DocumentTermMatrix(corpuspp[doc.lengths > 0])
```

```{r}
#LDA model with 5 topics selected
lda_5pp = LDA(dtmpp, k = 5, method = 'Gibbs', control = list(seed = list(1505)))

#LDA model with 2 topics selected
lda_2pp = LDA(dtmpp, k = 2, method = 'Gibbs', control = list(seed = list(1505)))

#LDA model with 10 topics selected
lda_10pp = LDA(dtmpp, k = 10, method = 'Gibbs', control = list(seed = list(1505)))
```

##### Probabilidad de que una palabra pertenezca a un tópico

```{r}
lda_td1pp <- tidy(lda_2pp, matrix = "beta")
lda_td2pp <- tidy(lda_5pp, matrix = "beta")
lda_td3pp <- tidy(lda_10pp, matrix = "beta")
```

```{r}
terminos_frecuentes1pp <- lda_td1pp %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes2pp <- lda_td2pp %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes3pp <- lda_td3pp %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
```


#### Podemos

```{r}
corpuspod <- Corpus(VectorSource(datospodemos$token))

doc.lengths <- rowSums(as.matrix(DocumentTermMatrix(corpuspod)))
dtmpod <- DocumentTermMatrix(corpuspod[doc.lengths > 0])
```

```{r}
#LDA model with 5 topics selected
lda_5pod = LDA(dtmpod, k = 5, method = 'Gibbs', control = list(seed = list(1505)))

#LDA model with 2 topics selected
lda_2pod = LDA(dtmpod, k = 2, method = 'Gibbs', control = list(seed = list(1505)))

#LDA model with 10 topics selected
lda_10pod = LDA(dtmpod, k = 10, method = 'Gibbs', control = list(seed = list(1505)))
```

## Probabilidad de que un tweet pertenzeca a un tópico

```{r}
lda_td1pod <- tidy(lda_2pod, matrix = "beta")
lda_td2pod <- tidy(lda_5pod, matrix = "beta")
lda_td3pod <- tidy(lda_10pod, matrix = "beta")
```

```{r}
terminos_frecuentes1pod <- lda_td1pod %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes2pod <- lda_td2pod %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
terminos_frecuentes3pod <- lda_td3pod %>% group_by(topic) %>% top_n(5, beta) %>% ungroup() %>% arrange(topic, -beta)
```














